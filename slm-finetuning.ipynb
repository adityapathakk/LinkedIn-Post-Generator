{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:15:21.295663Z",
     "iopub.status.busy": "2025-05-25T09:15:21.295427Z",
     "iopub.status.idle": "2025-05-25T09:16:49.870524Z",
     "shell.execute_reply": "2025-05-25T09:16:49.869517Z",
     "shell.execute_reply.started": "2025-05-25T09:15:21.295640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch datasets accelerate bitsandbytes scikit-learn evaluate rouge_score\n",
    "# evaluate, rouge_score weren't used later due to lack of GPU memory on Kaggle (more than 16GB used) \n",
    "!pip install -q --upgrade transformers datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:16:49.872620Z",
     "iopub.status.busy": "2025-05-25T09:16:49.872342Z",
     "iopub.status.idle": "2025-05-25T09:16:55.042004Z",
     "shell.execute_reply": "2025-05-25T09:16:55.041184Z",
     "shell.execute_reply.started": "2025-05-25T09:16:49.872592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "from transformers.utils.logging import enable_progress_bar\n",
    "enable_progress_bar()\n",
    "\n",
    "import datasets\n",
    "datasets.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-25T09:16:55.043144Z",
     "iopub.status.busy": "2025-05-25T09:16:55.042811Z",
     "iopub.status.idle": "2025-05-25T09:16:55.744447Z",
     "shell.execute_reply": "2025-05-25T09:16:55.743815Z",
     "shell.execute_reply.started": "2025-05-25T09:16:55.043120Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# create working directories\n",
    "os.makedirs(\"/kaggle/working/data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:16:55.745947Z",
     "iopub.status.busy": "2025-05-25T09:16:55.745442Z",
     "iopub.status.idle": "2025-05-25T09:16:55.755478Z",
     "shell.execute_reply": "2025-05-25T09:16:55.754799Z",
     "shell.execute_reply.started": "2025-05-25T09:16:55.745926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# defining templates for enrichment\n",
    "templates = [\n",
    "    \"preprocess: {sent} ğŸ˜Š\",\n",
    "    \"preprocess: ğŸ‰ {sent} Hereâ€™s why it mattersâ€¦\",\n",
    "    \"preprocess: ğŸ’¡ {sent} What I took awayâ€¦\",\n",
    "    \"preprocess: ğŸ” {sent} Key point:\",\n",
    "    \"preprocess: ğŸš€ {sent} Ready to take off!\",\n",
    "    \"preprocess: ğŸ“£ {sent} Spread the word!\",\n",
    "    \"preprocess: ğŸ“ Quick recap: {sent}\",\n",
    "    \"preprocess: ğŸ¯ Key takeaway: {sent}\",\n",
    "    \"preprocess: ğŸ”¥ {sent} My top 3 insightsâ€¦\",\n",
    "    \"preprocess: ğŸŒŸ {sent} Shine bright today!\",\n",
    "    \"preprocess: ğŸ’¬ {sent} Would love your thoughts.\",\n",
    "    \"preprocess: ğŸ¤” {sent} What do you think?\",\n",
    "    \"preprocess: âœ¨ {sent} Hereâ€™s what stood outâ€¦\",\n",
    "    \"preprocess: ğŸŒ± {sent} Growth moment:\",\n",
    "    \"preprocess: ğŸ™ï¸ {sent} Hereâ€™s the storyâ€¦\",\n",
    "    \"preprocess: âœ… {sent} Lesson learned.\",\n",
    "    \"preprocess: ğŸ’­ Reflecting: {sent}\",\n",
    "    \"preprocess: ğŸ“ˆ {sent} Trends update!\",\n",
    "    \"preprocess: ğŸ› ï¸ {sent} How to apply:\",\n",
    "    \"preprocess: ğŸ’¼ {sent} Business angle:\",\n",
    "    \"preprocess: ğŸŒ {sent} Global perspective:\",\n",
    "    \"preprocess: ğŸ’ {sent} Gem of insight:\",\n",
    "    \"preprocess: ğŸ† {sent} Achievement unlocked:\",\n",
    "    \"preprocess: ğŸ“Š {sent} Data speaks:\",\n",
    "    \"preprocess: ğŸ¥ {sent} Watch now:\",\n",
    "    \"preprocess: ğŸ“š {sent} Key learnings:\",\n",
    "    \"preprocess: ğŸ¤ {sent} Letâ€™s collaborate!\",\n",
    "    \"preprocess: ğŸ”— {sent} Check the link below:\",\n",
    "    \"preprocess: ğŸ•’ {sent} Time check:\",\n",
    "    \"preprocess: ğŸ“… {sent} Save the date:\",\n",
    "    \"preprocess: ğŸ”„ {sent} Updated insights:\",\n",
    "    \"preprocess: ğŸ§  {sent} Food for thought:\",\n",
    "    \"preprocess: ğŸŒŸ {sent} Spotlight:\",\n",
    "    \"preprocess: ğŸŒ± {sent} Growth hack:\",\n",
    "    \"preprocess: ğŸ¯ {sent} Laser focus:\",\n",
    "    \"preprocess: ğŸ” {sent} Deep dive:\",\n",
    "    \"preprocess: {sent} Here's what I learned.\",\n",
    "    \"preprocess: Reflecting on {sent}, I've noted several insights.\",\n",
    "    \"preprocess: Recently, {sent}. Let me explain further.\",\n",
    "    \"preprocess: Delighted to share that {sent}.\",\n",
    "    \"preprocess: {sent} Key takeaways below.\",\n",
    "    \"preprocess: Had the opportunity to {sent}; here are my reflections.\",\n",
    "    \"preprocess: {sent}. Here's why it's significant professionally.\",\n",
    "    \"preprocess: An insightful experience: {sent}.\",\n",
    "    \"preprocess: {sent} Here are the core ideas.\",\n",
    "    \"preprocess: Exploring the topic of {sent}; here's my perspective.\",\n",
    "    \"preprocess: I recently encountered {sent}. Hereâ€™s what stood out.\",\n",
    "    \"preprocess: Sharing thoughts on {sent}.\",\n",
    "    \"preprocess: A professional milestone: {sent}.\",\n",
    "    \"preprocess: {sent} Letâ€™s discuss the implications.\",\n",
    "    \"preprocess: Recently focused on {sent}. Here are valuable insights.\",\n",
    "    \"preprocess: {sent} Here are strategies that worked.\",\n",
    "    \"preprocess: {sent} This matters because:\",\n",
    "    \"preprocess: {sent} Highlighting key points:\",\n",
    "    \"preprocess: {sent} Professional reflections follow.\",\n",
    "    \"preprocess: Had an insightful discussion about {sent}. Hereâ€™s what emerged.\",\n",
    "    \"preprocess: Attended an insightful session on {sent}. My main takeaways:\",\n",
    "    \"preprocess: Recently presented about {sent}; here are some essential points.\",\n",
    "    \"preprocess: Sharing professional learnings from {sent}.\",\n",
    "    \"preprocess: {sent} Why does it matter professionally?\",\n",
    "    \"preprocess: Gained clarity around {sent}; hereâ€™s how.\",\n",
    "    \"preprocess: Key professional insights gained from {sent}:\",\n",
    "    \"preprocess: A valuable perspective on {sent}:\",\n",
    "    \"preprocess: Thought leadership insight: {sent}\",\n",
    "    \"preprocess: Recently engaged in {sent}. Professional lessons learned:\",\n",
    "    \"preprocess: Professional insight into {sent}:\",\n",
    "    \"preprocess: Exploring implications of {sent}.\",\n",
    "    \"preprocess: {sent} Critical professional considerations:\",\n",
    "    \"preprocess: Discussing the importance of {sent}.\",\n",
    "    \"preprocess: Key observations regarding {sent}:\",\n",
    "    \"preprocess: Sharing experiences about {sent}:\",\n",
    "    \"preprocess: My professional perspective on {sent}:\",\n",
    "    \"preprocess: {sent} Essential reflections:\",\n",
    "    \"preprocess: Addressing the professional relevance of {sent}.\",\n",
    "    \"preprocess: {sent} Hereâ€™s a summary of the key insights.\",\n",
    "    \"preprocess: Key professional learnings from {sent}.\",\n",
    "    \"preprocess: Important discussion points regarding {sent}:\",\n",
    "    \"preprocess: Strategic insights from {sent}:\",\n",
    "    \"preprocess: {sent} What professionals should consider:\",\n",
    "    \"preprocess: Sharing a professional viewpoint on {sent}.\",\n",
    "    \"preprocess: Professional insights derived from {sent}.\",\n",
    "    \"preprocess: Analyzing the professional impact of {sent}.\",\n",
    "    \"preprocess: Highlighting important outcomes from {sent}.\",\n",
    "    \"preprocess: Professional reflection on {sent}.\",\n",
    "    \"preprocess: My professional takeaways from {sent}.\",\n",
    "    \"preprocess: Key insights from my recent experience with {sent}.\",\n",
    "    \"preprocess: Important findings about {sent}.\",\n",
    "    \"preprocess: Professional review of {sent}.\",\n",
    "    \"preprocess: Sharing detailed insights about {sent}.\",\n",
    "    \"preprocess: Professional thoughts on {sent}:\",\n",
    "    \"preprocess: Insightful outcomes from {sent}.\",\n",
    "    \"preprocess: Professional perspective following {sent}:\",\n",
    "    \"preprocess: Reflecting professionally on {sent}.\",\n",
    "    \"preprocess: Discussing professional experiences with {sent}.\",\n",
    "    \"preprocess: Essential lessons from {sent}.\",\n",
    "    \"preprocess: Professional commentary on {sent}.\",\n",
    "    \"preprocess: Evaluating the professional significance of {sent}.\",\n",
    "    \"preprocess: Key reflections about {sent}.\",\n",
    "    \"preprocess: Strategic professional insights from {sent}.\",\n",
    "    \"preprocess: Professional development lessons from {sent}.\",\n",
    "    \"preprocess: Sharing a professional summary of {sent}.\",\n",
    "    \"preprocess: Professional insights: {sent}.\",\n",
    "    \"preprocess: My recent experience with {sent}; important outcomes:\",\n",
    "    \"preprocess: Professional analysis of {sent}.\",\n",
    "    \"preprocess: Reflecting on professional growth via {sent}.\",\n",
    "    \"preprocess: Insights gained professionally from {sent}.\",\n",
    "    \"preprocess: Professional experience highlights: {sent}.\",\n",
    "    \"preprocess: Discussing the professional implications of {sent}.\",\n",
    "    \"preprocess: Professional considerations from {sent}.\",\n",
    "    \"preprocess: A professional review of {sent}.\",\n",
    "    \"preprocess: Important professional takeaways from {sent}.\",\n",
    "    \"preprocess: Highlighting professional observations about {sent}.\",\n",
    "    \"preprocess: Professional insights on {sent}.\",\n",
    "    \"preprocess: Thought-provoking professional findings from {sent}.\",\n",
    "    \"preprocess: Strategic considerations regarding {sent}.\",\n",
    "    \"preprocess: My professional reflection on {sent}.\",\n",
    "    \"preprocess: Detailed professional insights from {sent}.\",\n",
    "    \"preprocess: A professional reflection about {sent}.\",\n",
    "    \"preprocess: Insightful professional lessons from {sent}.\",\n",
    "    \"preprocess: Discussing key professional outcomes from {sent}.\",\n",
    "    \"preprocess: Critical professional insights: {sent}.\",\n",
    "    \"preprocess: Strategic professional perspective on {sent}.\",\n",
    "    \"preprocess: Essential professional insights about {sent}.\",\n",
    "    \"preprocess: My analysis and insights from {sent}.\",\n",
    "    \"preprocess: Professional reflections regarding {sent}.\",\n",
    "    \"preprocess: Examining professional learnings from {sent}.\",\n",
    "    \"preprocess: Discussing the professional takeaways from {sent}.\",\n",
    "    \"preprocess: Sharing important professional learnings from {sent}.\",\n",
    "    \"preprocess: Summarizing my professional experience with {sent}.\",\n",
    "    \"preprocess: Valuable professional insights regarding {sent}.\",\n",
    "    \"preprocess: A reflection on professional growth through {sent}.\",\n",
    "    \"preprocess: Analyzing professional implications of {sent}.\",\n",
    "    \"preprocess: Professional perspectives gained from {sent}.\",\n",
    "    \"preprocess: Sharing insights from my recent professional experience: {sent}.\",\n",
    "    \"preprocess: Insights and reflections about {sent} from a professional viewpoint.\",\n",
    "    \"preprocess: Discussing the professional insights gained from {sent}.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:16:55.757380Z",
     "iopub.status.busy": "2025-05-25T09:16:55.757175Z",
     "iopub.status.idle": "2025-05-25T09:17:13.534714Z",
     "shell.execute_reply": "2025-05-25T09:17:13.534022Z",
     "shell.execute_reply.started": "2025-05-25T09:16:55.757363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3927b45889154d65a2015ebcc8e45be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7c5ef12d9b4396b5872ed45caefc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f1c6219b0548edae4aa44916857970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/171k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be68fbf87a34e38a6f4c8af9cfcebaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/31.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train â†’ /kaggle/working/data/train.jsonl\n",
      "Saved validation â†’ /kaggle/working/data/validation.jsonl\n",
      "Saved test â†’ /kaggle/working/data/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# loading CommonGen train+validation split\n",
    "cg = load_dataset(\"common_gen\", split=\"train+validation\")\n",
    "\n",
    "# generating pairs\n",
    "pairs = []\n",
    "for ex in cg:\n",
    "    raw = \", \".join(ex[\"concepts\"])\n",
    "    sent = ex[\"target\"]\n",
    "    tmpl = random.choice(templates)\n",
    "    refined = tmpl.format(sent=sent)\n",
    "    pairs.append({\"raw\": raw, \"refined\": refined})\n",
    "\n",
    "# splitting into train/val/test (80/10/10)\n",
    "train_val, test = train_test_split(pairs, test_size=0.10, random_state=42)\n",
    "train, val    = train_test_split(train_val, test_size=0.11, random_state=42)  # ~10% of original\n",
    "\n",
    "# saving to JSONL\n",
    "for split, data in ((\"train\", train), (\"validation\", val), (\"test\", test)):\n",
    "    path = f\"/kaggle/working/data/{split}.jsonl\"\n",
    "    with open(path, \"w\") as f:\n",
    "        for ex in data:\n",
    "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {split} â†’ {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:22:24.538205Z",
     "iopub.status.busy": "2025-05-25T11:22:24.537628Z",
     "iopub.status.idle": "2025-05-25T11:22:27.922513Z",
     "shell.execute_reply": "2025-05-25T11:22:27.921726Z",
     "shell.execute_reply.started": "2025-05-25T11:22:24.538180Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# loading JSONL datasets\n",
    "data_files = {\n",
    "    \"train\": \"/kaggle/working/data/train.jsonl\",\n",
    "    \"validation\": \"/kaggle/working/data/validation.jsonl\"\n",
    "}\n",
    "ds = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# initializing tokenizer & model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer  = T5Tokenizer.from_pretrained(model_name)\n",
    "model      = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# preprocessing function\n",
    "def preprocess_fn(ex):\n",
    "    inp = \"preprocess: \" + ex[\"raw\"]\n",
    "    tgt = ex[\"refined\"]\n",
    "    mi  = tokenizer(inp,  max_length=64, truncation=True, padding=\"max_length\")\n",
    "    lab = tokenizer(tgt,  max_length=64, truncation=True, padding=\"max_length\")\n",
    "    mi[\"labels\"] = lab[\"input_ids\"]\n",
    "    return mi\n",
    "\n",
    "tokenized = ds.map(preprocess_fn, batched=False, remove_columns=[\"raw\",\"refined\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:36:00.414829Z",
     "iopub.status.busy": "2025-05-25T12:36:00.414283Z",
     "iopub.status.idle": "2025-05-25T13:41:08.816150Z",
     "shell.execute_reply": "2025-05-25T13:41:08.815281Z",
     "shell.execute_reply.started": "2025-05-25T12:36:00.414802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 57,196\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 17,880\n",
      "  Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17880' max='17880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17880/17880 1:05:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.102200</td>\n",
       "      <td>0.957670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.037300</td>\n",
       "      <td>0.884950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.959200</td>\n",
       "      <td>0.828353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.908900</td>\n",
       "      <td>0.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.875400</td>\n",
       "      <td>0.759752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>0.739392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.724401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.810100</td>\n",
       "      <td>0.713164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.794800</td>\n",
       "      <td>0.704986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.781900</td>\n",
       "      <td>0.698115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.773500</td>\n",
       "      <td>0.691307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.686832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.759800</td>\n",
       "      <td>0.683342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.752300</td>\n",
       "      <td>0.678941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.748700</td>\n",
       "      <td>0.675550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.672902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.740100</td>\n",
       "      <td>0.670146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.739100</td>\n",
       "      <td>0.667785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.666091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.730300</td>\n",
       "      <td>0.664193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.662839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.727400</td>\n",
       "      <td>0.661689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.724800</td>\n",
       "      <td>0.660278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.659542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.722200</td>\n",
       "      <td>0.658544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.724400</td>\n",
       "      <td>0.657927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.716400</td>\n",
       "      <td>0.657395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.718500</td>\n",
       "      <td>0.656573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.719600</td>\n",
       "      <td>0.656311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.717100</td>\n",
       "      <td>0.656341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.719700</td>\n",
       "      <td>0.655986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.655921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.655840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.655824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.717600</td>\n",
       "      <td>0.655838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-2000\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-2000/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-2000/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-2000/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-2000/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-2000/added_tokens.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-4000\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-4000/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-4000/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-4000/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-4000/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-4000/added_tokens.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-6000\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-6000/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-6000/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-6000/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-6000/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-6000/added_tokens.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-8000\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-8000/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-8000/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-8000/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-8000/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-8000/added_tokens.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-10000\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-10000/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-10000/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-10000/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-10000/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-10000/added_tokens.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-12000\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-12000/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-12000/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-12000/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-12000/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-12000/added_tokens.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-14000\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-14000/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-14000/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-14000/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-14000/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-14000/added_tokens.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-16000\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-16000/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-16000/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-16000/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-16000/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-16000/added_tokens.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7070\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_ckpt/checkpoint-17880\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-17880/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_ckpt/checkpoint-17880/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_ckpt/checkpoint-17880/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-17880/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-17880/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_ckpt/checkpoint-17880/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /kaggle/working/slm_preproc_ckpt/checkpoint-16000 (score: 0.6558241248130798).\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Saving model checkpoint to /kaggle/working/slm_preproc_final\n",
      "Configuration saved in /kaggle/working/slm_preproc_final/config.json\n",
      "Configuration saved in /kaggle/working/slm_preproc_final/generation_config.json\n",
      "Model weights saved in /kaggle/working/slm_preproc_final/model.safetensors\n",
      "tokenizer config file saved in /kaggle/working/slm_preproc_final/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/slm_preproc_final/special_tokens_map.json\n",
      "added tokens file saved in /kaggle/working/slm_preproc_final/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/slm_preproc_ckpt\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    save_steps=2000,\n",
    "    fp16=True, # GPU-enabled on Kaggle Jupyter Notebook\n",
    "    disable_tqdm=False,\n",
    "    report_to=[\"none\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"/kaggle/working/slm_preproc_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T13:41:08.817867Z",
     "iopub.status.busy": "2025-05-25T13:41:08.817631Z",
     "iopub.status.idle": "2025-05-25T13:41:10.163214Z",
     "shell.execute_reply": "2025-05-25T13:41:10.162584Z",
     "shell.execute_reply.started": "2025-05-25T13:41:08.817850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /kaggle/working/slm_preproc_final/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file /kaggle/working/slm_preproc_final/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file /kaggle/working/slm_preproc_final/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /kaggle/working/slm_preproc_final.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file /kaggle/working/slm_preproc_final/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> RAW: branding, college, talk\n",
      "â†’ preprocess: Discussing the professional takeaways from students talk about branding at a college ..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> RAW: launch, product, next week\n",
      "â†’ preprocess: Discussing the professional takeaways from the launch of the product next week ..\n",
      "\n",
      "> RAW: sustainable design, workshop, attendees\n",
      "â†’ preprocess: Discussing the professional insights gained from attendees attended a workshop on sustainable design ..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"/kaggle/working/slm_preproc_final\",\n",
    "    tokenizer=\"/kaggle/working/slm_preproc_final\",\n",
    "    device=0  # GPU\n",
    ")\n",
    "\n",
    "for raw in [\n",
    "    \"branding, college, talk\",\n",
    "    \"launch, product, next week\",\n",
    "    \"sustainable design, workshop, attendees\"\n",
    "]:\n",
    "    out = pipe(\"preprocess: \" + raw, max_length=64)[0][\"generated_text\"]\n",
    "    print(f\"> RAW: {raw}\\nâ†’ {out}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
